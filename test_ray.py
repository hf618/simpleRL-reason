import ray
import os
import subprocess
import torch

# 1. å®šä¹‰ä¸€ä¸ªè¿œç¨‹ä»»åŠ¡ï¼Œæ‰“å°ç¯å¢ƒã€CUDAå¯ç”¨æ€§ã€å¹¶å°è¯•ç¼–è¯‘æŸä¸ªéœ€è¦CUDAçš„æ“ä½œ
@ray.remote(num_gpus=1)
def test_cuda_in_ray():
    print("PID:", os.getpid())
    
    # æ‰“å° LD_LIBRARY_PATHï¼Œç¡®è®¤æ˜¯å¦ä¼ å…¥æˆåŠŸ
    print("\n[Env] LD_LIBRARY_PATH =", os.getenv("LD_LIBRARY_PATH", "Not set"))

    # æ£€æŸ¥ PyTorch æ˜¯å¦èƒ½çœ‹åˆ° CUDA
    print("\n[PyTorch] CUDA available:", torch.cuda.is_available())

    if torch.cuda.is_available():
        print("[PyTorch] Current device:", torch.cuda.current_device())
        print("[PyTorch] Device name:", torch.cuda.get_device_name(torch.cuda.current_device()))
    else:
        print("âš ï¸ PyTorch cannot find CUDA")

    # å¯é€‰ï¼šè¿è¡Œä¸€ä¸ªç®€å•çš„ CUDA æ“ä½œæ¥éªŒè¯
    try:
        x = torch.randn(3, 3).cuda()
        y = torch.randn(3, 3).cuda()
        z = torch.matmul(x, y)
        print("\n[Matmul] Result on CUDA:", z)
    except Exception as e:
        print("ğŸš¨ CUDA op failed:", str(e))

    # å¯é€‰ï¼šè¿è¡Œ ldconfig æŸ¥è¯¢ libcuda.so
    try:
        result = subprocess.check_output(["ldconfig", "-p", "|", "grep", "libcuda"])
        print("\n[ldconfig] Found libcuda:\n", result.decode())
    except Exception as e:
        print("ğŸš¨ ldconfig failed to find libcuda:", str(e))


if __name__ == "__main__":
    # è®¾ç½® LD_LIBRARY_PATHï¼ˆæ ¹æ®ä½ çš„æƒ…å†µä¿®æ”¹ï¼‰
    ld_path = "/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6/targets/x86_64-linux/lib"

    # å¯åŠ¨ Rayï¼Œå¸¦ç¯å¢ƒå˜é‡ä¼ é€’
    ray.init(
        runtime_env={
            "env_vars": {
                "LD_LIBRARY_PATH": ld_path
            }
        }
    )

    # æäº¤ä»»åŠ¡
    ray.get(test_cuda_in_ray.remote())